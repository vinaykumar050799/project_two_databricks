{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a879d02-4b61-4602-a507-2804fef0efbc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./01-config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "274a98a6-8983-48d2-b3b1-4d592059e096",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Upserter():\n",
    "    def __init__(self,temp_view_name,merge_query):\n",
    "        self.temp_view_name = temp_view_name\n",
    "        self.merge_query = merge_query\n",
    "    \n",
    "    def upsertdata(self,batch_data,batch_id):\n",
    "        batch_data.createOrReplaceTempView(self.temp_view_name)\n",
    "        # print(self.merge_query)\n",
    "        # spark.sql(self.merge_query)\n",
    "        batch_data._jdf.sparkSession().sql(self.merge_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "13d8fc76-1b57-48ad-8e9b-b365acb7fc38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Gold():\n",
    "    def __init__(self,env):\n",
    "        Conf  = Config()\n",
    "        self.checkpoint_base = Conf.base_dir_checkpoint + \"/checkpoints\" \n",
    "        self.base_data_dir_test = Conf.base_dir_data + \"/test_data\"\n",
    "        self.catalog = env\n",
    "        self.silver_db = Conf.silver_db_name\n",
    "        self.gold_db = Conf.gold_db_name\n",
    "        spark.sql(f\"USE {self.catalog}.{self.gold_db}\")\n",
    "    \n",
    "    def upsert_workout_bpm_summary(self,once=False,process_time=\"10 seconds\",startingVersion=0):\n",
    "        import pyspark.sql.functions as f\n",
    "\n",
    "        merge_query=f\"\"\"\n",
    "        MERGE INTO {self.catalog}.{self.gold_db}.workout_bpm_summary a USING workout_bpm_summary_delta b ON\n",
    "        a.user_id = b.user_id AND a.session_id = b.session_id AND a.workout_id = b.workout_id\n",
    "        WHEN NOT MATCHED THEN INSERT * \n",
    "        \"\"\"\n",
    "\n",
    "        data_upserter = Upserter(\"workout_bpm_summary_delta\",merge_query)\n",
    "\n",
    "        user_bins_tbl=spark.read.table(f\"{self.catalog}.{self.silver_db}.user_bins\")\n",
    "\n",
    "        read_stream = (spark.readStream\n",
    "                       .option(\"startingVersion\",startingVersion)\n",
    "                       .table(f\"{self.catalog}.{self.silver_db}.workout_bpm\")\n",
    "                       .withWatermark(\"end_time\",\"30 seconds\")\n",
    "                       .groupBy(\"workout_id\",\"session_id\",\"user_id\",\"end_time\")\n",
    "                       .agg(f.min(\"heartrate\").alias(\"min_bpm\"),f.avg(\"heartrate\").alias(\"avg_bpm\"),f.max(\"heartrate\").alias(\"max_bpm\")\n",
    "                            ,f.count(\"heartrate\").alias(\"num_recordings\"))\n",
    "                       .join(user_bins_tbl,[\"user_id\"])\n",
    "                       .select(\"workout_id\",\"session_id\",\"user_id\",\"age\",\"gender\",\"city\",\"state\",\"min_bpm\",\"avg_bpm\",\"max_bpm\",\"num_recordings\")\n",
    "                       )\n",
    "        stream_writer = (read_stream.writeStream\n",
    "                                 .foreachBatch(data_upserter.upsertdata)\n",
    "                                 .outputMode(\"append\")\n",
    "                                 .option(\"checkpointLocation\", f\"{self.checkpoint_base}/workout_bpm_summary\")\n",
    "                                 .queryName(\"workout_bpm_summary_upsert_stream\")\n",
    "                        )\n",
    "        \n",
    "        if once:\n",
    "            stream_writer.trigger(availableNow=True).start()\n",
    "        else:\n",
    "            stream_writer.trigger(processingTime=process_time).start()\n",
    "        \n",
    "    def upsert(self,once=True,process_time=\"10 seconds\"):\n",
    "        import time\n",
    "        start = int(time.time())\n",
    "        print(f\"\\nExecuting gold layer upsert ...\")\n",
    "        self.upsert_workout_bpm_summary(once=once,process_time=process_time)\n",
    "        if once:\n",
    "            for stream in spark.streams.active:\n",
    "                stream.awaitTermination()\n",
    "        print(f\"Completed gold layer upsert {int(time.time()) - start} seconds\")\n",
    "    \n",
    "    def assert_count(self,table_name,expected_count,filter=\"true\"):\n",
    "        actual_count = spark.read.table(f\"{self.catalog}.{self.gold_db}.{table_name}\").where(filter).count()\n",
    "        assert expected_count == actual_count, f\"Expected {expected_count:,} records, found {actual_count:,} in {table_name} where {filter}\" \n",
    "        print(f\"Found {actual_count:,} / Expected {expected_count:,} records where {filter}: Success\")\n",
    "\n",
    "    def assert_rows(self, location, table_name, sets):\n",
    "        print(f\"Validating records in {table_name}...\", end='')\n",
    "        expected_rows = spark.read.format(\"parquet\").load(f\"{self.base_data_dir_test}/{location}_{sets}.parquet\").collect()\n",
    "        actual_rows = spark.table(table_name).collect()\n",
    "        assert expected_rows == actual_rows, f\"Expected data mismatches with the actual data in {table_name}\"\n",
    "        print(f\"Expected data matches with the actual data in {table_name}: Success\")\n",
    "        \n",
    "        \n",
    "    def validate(self, sets):\n",
    "        import time\n",
    "        start = int(time.time())\n",
    "        print(f\"\\nValidating gold layer records...\" )       \n",
    "        self.assert_rows(\"7-gym_summary\", \"gym_summary\", sets)       \n",
    "        if sets>1:\n",
    "            self.assert_count(\"workout_bpm_summary\", 2)\n",
    "        print(f\"Gold layer validation completed in {int(time.time()) - start} seconds\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5161706622636176,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "06-Gold",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
