{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a53a6711-6400-4843-98c8-5cdce83f9e57",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./01-config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "382a61e1-a7b1-467b-8ff0-8dc4acc0d7f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Bronze():   \n",
    "    def __init__(self, env):\n",
    "        Conf = Config()\n",
    "        self.landing_zone = Conf.base_dir_data + \"/raw\"\n",
    "        self.checkpoint_base = Conf.base_dir_checkpoint + \"/checkpoints\"        \n",
    "        self.catalog = env\n",
    "        self.bronze_db = Conf.bronze_db_name\n",
    "        self.silver_db = Conf.silver_db_name\n",
    "        spark.sql(f\"USE {self.catalog}.{self.bronze_db}\")\n",
    "    \n",
    "    def load_registered_users_bz(self,once=False,process_time = \"S seconds\"):\n",
    "        import pyspark.sql.functions as f\n",
    "        schema_ddl = \"user_id LONG,\tdevice_id LONG,\tmac_address\tSTRING, registration_timestamp DOUBLE\"\n",
    "\n",
    "        read_stream = (spark.readStream.format(\"cloudFiles\")\n",
    "                       .option(\"cloudFiles.format\",\"csv\")\n",
    "                       .option(\"maxfilesPerTrigger\",1)\n",
    "                       .schema(schema_ddl)\n",
    "                       .option(\"header\",\"true\")\n",
    "                       .load(f\"{self.landing_zone}/registered_users/\")\n",
    "                       .withColumn(\"load_time\",f.current_timestamp())\n",
    "                       .withColumn(\"source_file\",f.input_file_name())\n",
    "                       )\n",
    "        write_stream = (read_stream.writeStream.format(\"delta\")\n",
    "                        .outputMode(\"append\")\n",
    "                        .option(\"checkPointLocation\",f\"{self.checkpoint_base}/load_registered_users_bz\")\n",
    "                        .option(\"queryName\",\"load_registered_users_bz\")\n",
    "                        )\n",
    "\n",
    "        if once:\n",
    "            return write_stream.trigger(availableNow=True).toTable(f\"{self.catalog}.{self.bronze_db}.registered_users_bz\")\n",
    "        else:\n",
    "            return write_stream.trigger(processingTime=process_time).toTable(f\"{self.catalog}.{self.bronze_db}.registered_users_bz\")\n",
    "        \n",
    "        spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"bronze_p2\")\n",
    "\n",
    "\n",
    "    def load_gym_logins_bz(self,once=False,process_time = \"5 seconds\"):\n",
    "        import pyspark.sql.functions as f\n",
    "        schema_ddl = \"mac_address STRING, gym LONG, login DOUBLE, logout DOUBLE\"\n",
    "\n",
    "        read_stream = (spark.readStream.format(\"cloudFiles\")\n",
    "                       .option(\"cloudFiles.format\",\"csv\")\n",
    "                       .option(\"maxfilesPerTrigger\",1)\n",
    "                       .schema(schema_ddl)\n",
    "                       .option(\"header\",\"true\")\n",
    "                       .load(f\"{self.landing_zone}/gym_logins/\")\n",
    "                       .withColumn(\"load_time\",f.current_timestamp())\n",
    "                       .withColumn(\"source_file\",f.input_file_name())\n",
    "                       )\n",
    "        write_stream = (read_stream.writeStream.format(\"delta\")\n",
    "                        .outputMode(\"append\")\n",
    "                        .option(\"checkPointLocation\",f\"{self.checkpoint_base}/load_gym_logins_bz\")\n",
    "                        .option(\"queryName\",\"load_gym_logins_bz\")\n",
    "                        )\n",
    "\n",
    "        if once:\n",
    "            return write_stream.trigger(availableNow=True).toTable(f\"{self.catalog}.{self.bronze_db}.gym_logins_bz\")\n",
    "        else:\n",
    "            return write_stream.trigger(processingTime=process_time).toTable(f\"{self.catalog}.{self.bronze_db}.gym_logins_bz\")\n",
    "        \n",
    "        spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"bronze_p2\")\n",
    "    \n",
    "    def load_kafka_multiplex_bz(self,once=False,process_time = \"5 seconds\"):\n",
    "        import pyspark.sql.functions as f\n",
    "        schema_ddl = \"key STRING,value STRING,topic STRING,partition BIGINT,offset BIGINT,timestamp BIGINT\"\n",
    "        date_lookup = spark.read.table(f\"{self.catalog}.{self.silver_db}.date_lookup\").select(\"date\",\"week_part\")\n",
    "\n",
    "\n",
    "        read_stream = (spark.readStream.format(\"cloudFiles\")\n",
    "                       .option(\"cloudFiles.format\",\"json\")\n",
    "                       .option(\"maxfilesPerTrigger\",1)\n",
    "                       .schema(schema_ddl)\n",
    "                       .load(f\"{self.landing_zone}/kafka_multiplex/\")\n",
    "                       .withColumn(\"load_time\",f.current_timestamp())\n",
    "                       .withColumn(\"source_file\",f.input_file_name())\n",
    "                       .join(f.broadcast(date_lookup),f.to_date((f.col(\"timestamp\")/1000).cast(\"TIMESTAMP\")) == date_lookup.date,\"left\")\n",
    "                       )\n",
    "        \n",
    "        write_stream = (read_stream.writeStream.format(\"delta\")\n",
    "                        .outputMode(\"append\")\n",
    "                        .option(\"checkPointLocation\",f\"{self.checkpoint_base}/load_kafka_multiplex_bz\")\n",
    "                        .option(\"queryName\",\"load_kafka_multiplex_bz\")\n",
    "                        )\n",
    "        \n",
    "        if once:\n",
    "            return write_stream.trigger(availableNow=True).toTable(f\"{self.catalog}.{self.bronze_db}.kafka_multiplex_bz\")\n",
    "        else:\n",
    "            return write_stream.trigger(processingTime=process_time).toTable(f\"{self.catalog}.{self.bronze_db}.kafka_multiplex_bz\")\n",
    "        \n",
    "        spark.sparkContext.setLocalProperty(\"spark.scheduler.pool\", \"bronze_p1\")\n",
    "\n",
    "        \n",
    "    def load_bronze(self,once=True,process_time=\"5 seconds\"):\n",
    "        import time\n",
    "        start = time.time()\n",
    "        print(\"loading data into bronze layer\")\n",
    "        self.load_gym_logins_bz(once,process_time)\n",
    "        self.load_registered_users_bz(once,process_time)\n",
    "        self.load_kafka_multiplex_bz(once,process_time)\n",
    "        if once:\n",
    "            for stream in spark.streams.active:\n",
    "                stream.awaitTermination()\n",
    "        end = time.time()\n",
    "        print(f\"time taken for loading data into bronze {int(end)-int(start)} seconds\")\n",
    "    \n",
    "    def assert_count(self,table_name,expected_count,filter=\"true\"):\n",
    "        actual_count = spark.read.table(f\"{self.catalog}.{self.bronze_db}.{table_name}\").where(filter).count()\n",
    "        print(f\"actual_count : {actual_count}\")\n",
    "        assert actual_count == expected_count, f\"Expected {expected_count} rows in {table_name}, found {actual_count}\"\n",
    "        print(f\"Expected {expected_count} rows in {table_name}, found {actual_count}\")\n",
    "\n",
    "    def validate(self,sets):\n",
    "        import time\n",
    "        start = time.time()\n",
    "        print(\"validating bronze layer records\")\n",
    "        self.assert_count(\"registered_users_bz\",5 if sets==1 else 10)\n",
    "        self.assert_count(\"gym_logins_bz\",8 if sets==1 else 16)\n",
    "        self.assert_count(\"kafka_multiplex_bz\",7 if sets==1 else 13,\"topic='user_info'\")\n",
    "        self.assert_count(\"kafka_multiplex_bz\",16 if sets==1 else 32,\"topic='workout'\")\n",
    "        self.assert_count(\"kafka_multiplex_bz\",sets*253801,\"topic='bpm'\")\n",
    "        end = time.time()\n",
    "        print(f\"time taken for validating data into bronze {int(end)-int(start)} seconds\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "04-Bronze",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
