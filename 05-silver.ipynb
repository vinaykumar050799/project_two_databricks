{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "710a90c9-8f22-4e93-a2e9-60175115249a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./01-config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "437de0af-9c2f-4e59-97d1-b9f3b496eeac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Upserter():\n",
    "    def __init__(self,temp_view_name,merge_query):\n",
    "        self.temp_view_name = temp_view_name\n",
    "        self.merge_query = merge_query\n",
    "    \n",
    "    def upsertdata(self,batch_data,batch_id):\n",
    "        batch_data.createOrReplaceTempView(self.temp_view_name)\n",
    "        # print(self.merge_query)\n",
    "        # spark.sql(self.merge_query)\n",
    "        batch_data._jdf.sparkSession().sql(self.merge_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "76632f30-67c5-4cb5-af0b-2edc8762f408",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class CDCUpserter():\n",
    "    def __init__(self,temp_view_name,merge_query,column_id,sort_id):\n",
    "        self.temp_view_name = temp_view_name\n",
    "        self.merge_query = merge_query\n",
    "        self.column_id = column_id\n",
    "        self.sort_id = sort_id\n",
    "    \n",
    "    def upsertdata(self,batch_data,batch_id):\n",
    "        import pyspark.sql.functions as f\n",
    "        from pyspark.sql.window import Window\n",
    "\n",
    "        window_frame = Window.partitionBy(f.col(self.column_id)).orderBy(f.col(self.sort_id).desc())\n",
    "        batch_data = batch_data.filter(\"update_type in ('new','update')\")\n",
    "        batch_data = batch_data.withColumn(\"rank\",f.rank().over(window_frame)).filter(\"rank=1\").drop(\"rank\")\n",
    "        batch_data.createOrReplaceTempView(self.temp_view_name)\n",
    "        batch_data._jdf.sparkSession().sql(self.merge_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b25f3cc-8136-40a0-9c89-6b3ab4aeaef6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class Silver():\n",
    "    def __init__(self,env):\n",
    "        Conf  = Config()\n",
    "        self.checkpoint_base = Conf.base_dir_checkpoint + \"/checkpoints\"        \n",
    "        self.catalog = env\n",
    "        self.bronze_db = Conf.bronze_db_name\n",
    "        self.silver_db = Conf.silver_db_name\n",
    "        spark.sql(f\"USE {self.catalog}.{self.silver_db}\")\n",
    "    \n",
    "    def upsert_user_details(self,once=False,process_time = \"5 seconds\",startingVersion=0):\n",
    "        merge_query = f\"\"\"MERGE INTO {self.catalog}.{self.silver_db}.users a \n",
    "        USING users_delta b ON a.user_id = b.user_id\n",
    "        WHEN NOT MATCHED THEN INSERT *\"\"\"\n",
    "\n",
    "        upsert = Upserter(f\"users_delta\",merge_query)    \n",
    "        \n",
    "        read_stream = (spark.readStream\n",
    "                       .option(\"startingVersion\",startingVersion)\n",
    "                       .option(\"ignoreDeletes\",True)\n",
    "                       .table(f\"{self.catalog}.{self.bronze_db}.registered_users_bz\")\n",
    "                       .selectExpr(\"user_id\",\"device_id\",\"mac_address\",\"cast(registration_timestamp as timestamp)\")\n",
    "                       .dropDuplicates([\"user_id\",\"device_id\"])\n",
    "                       )\n",
    "        \n",
    "        write_stream = (read_stream.writeStream\n",
    "                        .foreachBatch(upsert.upsertdata)\n",
    "                        .outputMode(\"update\")\n",
    "                        .option(\"queryName\",\"upsert_user_details_stream\")\n",
    "                        .option(\"checkpointLocation\",f\"{self.checkpoint_base}/user_details\")\n",
    "                        )\n",
    "        \n",
    "        if once:\n",
    "            return write_stream.trigger(availableNow=True).start()\n",
    "        else:\n",
    "            return write_stream.trigger(processingTime=process_time).start() \n",
    "        \n",
    "\n",
    "    def upsert_gym_logs(self,once=False,process_time = \"5 seconds\",startingVersion=0):\n",
    "        merge_query = f\"\"\"MERGE INTO {self.catalog}.{self.silver_db}.gym_logs a \n",
    "        USING gym_logs_delta b ON a.mac_address = b.mac_address AND a.gym = b.gym AND a.login = b.login \n",
    "        WHEN MATCHED AND b.logout>a.login AND b.logout>a.logout THEN UPDATE SET logout = b.logout \n",
    "        WHEN NOT MATCHED THEN INSERT *\"\"\"\n",
    "\n",
    "        upsert = Upserter(\"gym_logs_delta\",merge_query)    \n",
    "        \n",
    "        read_stream = (spark.readStream\n",
    "                       .option(\"startingVersion\",startingVersion)\n",
    "                       .option(\"ignoreDeletes\",True)\n",
    "                       .table(f\"{self.catalog}.{self.bronze_db}.gym_logins_bz\")\n",
    "                       .selectExpr(\"mac_address\",\"gym\",\"cast(login as timestamp)\",\"cast(logout as timestamp)\")\n",
    "                       .dropDuplicates([\"mac_address\",\"gym\",\"login\"])\n",
    "                       )\n",
    "        \n",
    "        write_stream = (read_stream.writeStream\n",
    "                        .foreachBatch(upsert.upsertdata)\n",
    "                        .outputMode(\"update\")\n",
    "                        .option(\"queryName\",\"upsert_gym_logs_stream\")\n",
    "                        .option(\"checkpointLocation\",f\"{self.checkpoint_base}/gym_logs\")\n",
    "                        )\n",
    "        \n",
    "        if once:\n",
    "            return write_stream.trigger(availableNow=True).start()\n",
    "        else:\n",
    "            return write_stream.trigger(processingTime=process_time).start()  \n",
    "    \n",
    "    def upsert_user_profile(self,once=False,process_time = \"5 seconds\",startingVersion=0):\n",
    "        import pyspark.sql.functions as f\n",
    "\n",
    "        #Idempotent - Insert new record\n",
    "        #           - Ignore deletes\n",
    "        #           - Update user details when\n",
    "        #               1. update_type in (\"new\", \"append\")\n",
    "        #               2. current update is newer than the earlier\n",
    "\n",
    "        schema = \"\"\"\n",
    "            user_id bigint, update_type STRING, timestamp FLOAT, \n",
    "            dob STRING, sex STRING, gender STRING, first_name STRING, last_name STRING, \n",
    "            address STRUCT<street_address: STRING, city: STRING, state: STRING, zip: INT>\"\"\"\n",
    "    \n",
    "        \n",
    "        merge_query = f\"\"\"MERGE INTO {self.catalog}.{self.silver_db}.user_profile a USING user_profile_delta b ON a.user_id = b.user_id \n",
    "        WHEN MATCHED AND a.updated < b.updated THEN UPDATE SET *\n",
    "            WHEN NOT MATCHED\n",
    "              THEN INSERT *\"\"\"\n",
    "\n",
    "        cdcupsert = CDCUpserter(\"user_profile_delta\",merge_query,\"user_id\",\"updated\")    \n",
    "        \n",
    "        read_stream = (spark.readStream\n",
    "                       .option(\"startingVersion\",startingVersion)\n",
    "                       .option(\"ignoreDeletes\",True)\n",
    "                       .table(f\"{self.catalog}.{self.bronze_db}.kafka_multiplex_bz\")\n",
    "                       .filter(\"topic='user_info'\")\n",
    "                       .select(f.from_json(f.col(\"value\").cast(\"string\"),schema).alias(\"v\"))\n",
    "                       .select(\"v.*\")\n",
    "                       .select(\"user_id\",f.to_date(\"dob\",\"MM/dd/yyyy\").alias(\"dob\"),\"sex\",\"gender\",\"first_name\",\"last_name\",\"address.street_address\",\"address.city\",\"address.state\",\"address.zip\",f.to_timestamp(\"timestamp\").alias(\"updated\"),\"update_type\")\n",
    "                       .withWatermark(\"updated\", \"30 seconds\")\n",
    "                       .dropDuplicates([\"user_id\",\"updated\"])\n",
    "                       )\n",
    "        \n",
    "        write_stream = (read_stream.writeStream\n",
    "                        .foreachBatch(cdcupsert.upsertdata)\n",
    "                        .outputMode(\"update\")\n",
    "                        .option(\"queryName\",\"upsert_user_profile_stream\")\n",
    "                        .option(\"checkpointLocation\",f\"{self.checkpoint_base}/user_profile\")\n",
    "                        )\n",
    "        \n",
    "        if once:\n",
    "            return write_stream.trigger(availableNow=True).start()\n",
    "        else:\n",
    "            return write_stream.trigger(processingTime=process_time).start()    \n",
    "    \n",
    "    def upsert_workouts(self,once=False,process_time = \"5 seconds\",startingVersion=0):\n",
    "        import pyspark.sql.functions as f\n",
    "        schema = \"user_id INT, workout_id INT,timestamp FLOAT, action STRING, session_id INT\"\n",
    "        \n",
    "        merge_query = f\"\"\"MERGE INTO {self.catalog}.{self.silver_db}.workouts a \n",
    "                            USING workouts_delta b ON a.user_id = b.user_id AND \n",
    "                            a.time=b.time WHEN NOT MATCHED THEN INSERT *\n",
    "                            \"\"\"\n",
    "\n",
    "        upsert = Upserter(\"workouts_delta\",merge_query)\n",
    "\n",
    "        #Idempotent - User cannot have two workout sessions at the same time. So ignore the duplicates and insert the new records\n",
    "\n",
    "        read_stream = (spark.readStream.option(\"startingVersion\",startingVersion)\n",
    "                       .option(\"ignoreDeletes\",True)\n",
    "                       .table(f\"{self.catalog}.{self.bronze_db}.kafka_multiplex_bz\")\n",
    "                       .filter(\"topic='workout'\")\n",
    "                       .select(f.from_json(f.col(\"value\").cast(\"string\"),schema).alias(\"v\"))\n",
    "                       .select(\"v.*\")\n",
    "                       .select(\"user_id\",\"workout_id\",f.col(\"timestamp\").cast(\"timestamp\").alias(\"time\"),\"action\",\"session_id\")\n",
    "                       .withWatermark(\"time\", \"30 seconds\")\n",
    "                       .dropDuplicates([\"user_id\", \"time\"])\n",
    "                       )\n",
    "        \n",
    "        write_stream = (read_stream.\n",
    "                        writeStream\n",
    "                        .foreachBatch(upsert.upsertdata)\n",
    "                        .outputMode(\"update\")\n",
    "                        .option(\"queryName\",\"upsert_workouts_stream\")\n",
    "                        .option(\"checkpointLocation\",f\"{self.checkpoint_base}/workouts\")\n",
    "                        )\n",
    "        \n",
    "        if once:\n",
    "            return write_stream.trigger(availableNow=True).start()\n",
    "        else:\n",
    "            return write_stream.trigger(processingTime=process_time).start()\n",
    "        \n",
    "    \n",
    "    def upsert_heart_rate(self,once=False,process_time = \"5 seconds\",startingVersion=0):\n",
    "        import pyspark.sql.functions as f\n",
    "        schema = \"device_id BIGINT, time TIMESTAMP, heartrate DOUBLE\"\n",
    "\n",
    "        merge_query = f\"\"\"MERGE INTO {self.catalog}.{self.silver_db}.heart_rate a \n",
    "        USING heart_rate_delta b ON a.device_id = b.device_id AND a.time=b.time \n",
    "        WHEN NOT MATCHED THEN INSERT *\"\"\"\n",
    "\n",
    "\n",
    "        upsert = Upserter(\"heart_rate_delta\",merge_query)\n",
    "\n",
    "        #Idempotent - Only one BPM signal is allowed at a timestamp. So ignore the duplicates and insert the new records\n",
    "\n",
    "        read_stream = (spark.readStream.option(\"startingVersion\",startingVersion)\n",
    "                       .option(\"ignoreDeletes\",True)\n",
    "                       .table(f\"{self.catalog}.{self.bronze_db}.kafka_multiplex_bz\")\n",
    "                       .filter(\"topic='bpm'\")\n",
    "                       .select(f.from_json(f.col(\"value\").cast(\"string\"),schema).alias(\"v\"))\n",
    "                       .select(\"v.*\",f.when(f.col(\"v.heartrate\")<=0,False).otherwise(True).alias(\"valid\"))\n",
    "                       .withWatermark(\"time\", \"30 seconds\")\n",
    "                       .dropDuplicates([\"device_id\", \"time\"])\n",
    "                       )\n",
    "        \n",
    "        write_stream = (read_stream.\n",
    "                        writeStream\n",
    "                        .foreachBatch(upsert.upsertdata)\n",
    "                        .outputMode(\"update\")\n",
    "                        .option(\"queryName\",\"upsert_heart_rate_stream\")\n",
    "                        .option(\"checkpointLocation\",f\"{self.checkpoint_base}/heart_rate\")\n",
    "                        )\n",
    "        \n",
    "        if once:\n",
    "            return write_stream.trigger(availableNow=True).start()\n",
    "        else:\n",
    "            return write_stream.trigger(processingTime=process_time).start()\n",
    "        \n",
    "\n",
    "    def get_user_age(self,dob_col):\n",
    "        import pyspark.sql.functions as f\n",
    "        age_col = f.floor((f.months_between(f.current_date(),dob_col))/12).alias(\"age\")\n",
    "        return (f.when(age_col<18,\"Under 18\")\n",
    "                .when(((age_col>=18)&(age_col<25)),\"18-25\")\n",
    "                .when(((age_col>=25)&(age_col<35)),\"25-35\")\n",
    "                .when(((age_col>=35)&(age_col<45)),\"35-45\")\n",
    "                .when(((age_col>=45)&(age_col<55)),\"45-55\")\n",
    "                .when(((age_col>=55)&(age_col<65)),\"55-65\")\n",
    "                .when(((age_col>=65)&(age_col<75)),\"65-75\")\n",
    "                .when(((age_col>=75)&(age_col<85)),\"75-85\")\n",
    "                .when(((age_col>=85)&(age_col<95)),\"85-95\")\n",
    "                .otherwise(\"95+\").alias(\"age\"))\n",
    "\n",
    "    def upsert_user_bins(self,once=False,process_time = \"5 seconds\",startingVersion=0):\n",
    "        import pyspark.sql.functions as f\n",
    "\n",
    "        merge_query = f\"\"\"\n",
    "                        MERGE INTO {self.catalog}.{self.silver_db}.user_bins a USING user_bins_delta b \n",
    "                        ON a.user_id = b.user_id WHEN MATCHED THEN UPDATE SET *\n",
    "                        WHEN NOT MATCHED THEN INSERT *\n",
    "                        \"\"\"\n",
    "\n",
    "        upserter = Upserter(\"user_bins_delta\",merge_query)\n",
    "        \n",
    "        users_tbl = spark.read.table(f\"{self.catalog}.{self.silver_db}.users\")\n",
    "\n",
    "        read_stream = (spark.readStream\n",
    "                       .option(\"ignoreChanges\",True)\n",
    "                       .option(\"startingVersion\",startingVersion)\n",
    "                       .table(f\"{self.catalog}.{self.silver_db}.user_profile\")\n",
    "                       )\n",
    "        read_user_details = read_stream.join(users_tbl,[\"user_id\"],\"left\").select(\"user_id\",self.get_user_age(f.col(\"dob\")),\"gender\",\"city\",\"state\")\n",
    "        \n",
    "        write_stream = (read_user_details.writeStream\n",
    "                        .foreachBatch(upserter.upsertdata)\n",
    "                        .outputMode(\"update\")\n",
    "                        .option(\"queryName\",\"upsert_user_bins_stream\")\n",
    "                        .option(\"checkpointLocation\",f\"{self.checkpoint_base}/user_bins\"))\n",
    "        \n",
    "        if once:\n",
    "            write_stream.trigger(availableNow=True).start()\n",
    "        else:\n",
    "            write_stream.trigger(processingTime=process_time).start()\n",
    "    \n",
    "    def upsert_completed_workouts(self,once=False,process_time = \"5 seconds\",startingVersion=0):\n",
    "        import pyspark.sql.functions as f\n",
    "\n",
    "        merge_query = f\"\"\"\n",
    "                    MERGE INTO {self.catalog}.{self.silver_db}.completed_workouts a USING completed_workouts_delta b\n",
    "                    ON a.user_id=b.user_id AND a.workout_id=b.workout_id AND a.session_id=b.session_id \n",
    "                    WHEN NOT MATCHED THEN INSERT *\n",
    "                    \"\"\"\n",
    "\n",
    "        upserter = Upserter(\"completed_workouts_delta\",merge_query)\n",
    "\n",
    "        df_stop = (spark.readStream\n",
    "                   .option(\"staringVersion\",startingVersion)\n",
    "                   .option(\"ignoreDeletes\",True)\n",
    "                   .table(f\"{self.catalog}.{self.silver_db}.workouts\")\n",
    "                   .withWatermark(\"time\",\"30 seconds\")\n",
    "                   .filter(\"action = 'stop'\")\n",
    "                   .selectExpr(\"user_id\",\"workout_id\",\"session_id\",\"action\",\"time as end_time\")\n",
    "                   \n",
    "                   )\n",
    "        df_start = (spark.readStream\n",
    "                   .option(\"staringVersion\",startingVersion)\n",
    "                   .option(\"ignoreDeletes\",True)\n",
    "                   .table(f\"{self.catalog}.{self.silver_db}.workouts\")\n",
    "                   .withWatermark(\"time\",\"30 seconds\")\n",
    "                   .filter(\"action = 'start'\")\n",
    "                   .selectExpr(\"user_id\",\"workout_id\",\"session_id\",\"action\",\"time as start_time\")\n",
    "                   )\n",
    "        \n",
    "        join_condition = [df_start.user_id==df_stop.user_id,df_start.workout_id==df_stop.workout_id,df_start.session_id==df_stop.session_id,df_stop.end_time < df_start.start_time + f.expr(\"INTERVAL 3 HOUR\")]\n",
    "\n",
    "        df_final = df_start.join(df_stop,join_condition,\"left\").select(df_start.user_id,df_start.workout_id,df_start.session_id,df_start.start_time,df_stop.end_time)\n",
    "\n",
    "        write_stream = (df_final.writeStream\n",
    "                        .foreachBatch(upserter.upsertdata)\n",
    "                        .outputMode(\"append\")\n",
    "                        .option(\"queryName\",\"upsert_completed_workouts_stream\")\n",
    "                        .option(\"checkpointLocation\",f\"{self.checkpoint_base}/completed_workouts\")\n",
    "                        )\n",
    "        \n",
    "        if once:\n",
    "            write_stream.trigger(availableNow=True).start()\n",
    "        else:\n",
    "            write_stream.trigger(processingTime=process_time).start()\n",
    "\n",
    "    \n",
    "    def upsert_workout_bpm(self,once=False,process_time = \"5 seconds\",startingVersion=0):\n",
    "        import pyspark.sql.functions as f\n",
    "\n",
    "        merge_query = f\"\"\"\n",
    "                    MERGE INTO {self.catalog}.{self.silver_db}.workout_bpm a USING workout_bpm_delta b\n",
    "                    ON a.user_id=b.user_id AND a.workout_id=b.workout_id AND a.session_id=b.session_id AND a.time=b.time\n",
    "                    WHEN NOT MATCHED THEN INSERT *\n",
    "                    \"\"\"\n",
    "\n",
    "        upserter = Upserter(\"workout_bpm_delta\",merge_query)\n",
    "\n",
    "        users_df = spark.read.table(f\"{self.catalog}.{self.silver_db}.users\")\n",
    "\n",
    "        df_complete_workouts = (spark.readStream\n",
    "                   .option(\"staringVersion\",startingVersion)\n",
    "                   .option(\"ignoreDeletes\",True)\n",
    "                   .table(f\"{self.catalog}.{self.silver_db}.completed_workouts\")\n",
    "                   .join(users_df,[\"user_id\"])\n",
    "                   .selectExpr(\"user_id\",\"device_id\",\"workout_id\",\"session_id\",\"start_time\",\"end_time\")\n",
    "                   .withWatermark(\"start_time\",\"30 seconds\")\n",
    "                   )\n",
    "        df_heart_rate = (spark.readStream\n",
    "                   .option(\"staringVersion\",startingVersion)\n",
    "                   .option(\"ignoreDeletes\",True)\n",
    "                   .table(f\"{self.catalog}.{self.silver_db}.heart_rate\")\n",
    "                   .selectExpr(\"device_id\",\"time\",\"heartrate\")\n",
    "                   .withWatermark(\"time\",\"30 seconds\")\n",
    "                   )\n",
    "        \n",
    "        join_condition = [df_complete_workouts.device_id==df_heart_rate.device_id, df_heart_rate.time>df_complete_workouts.start_time,df_heart_rate.time<=df_complete_workouts.end_time,df_complete_workouts.end_time < df_heart_rate.time + f.expr('interval 3 hour')]\n",
    "\n",
    "        df_final = df_heart_rate.join(df_complete_workouts,join_condition).select(\"user_id\",\"workout_id\",\"session_id\",\"start_time\",\"end_time\",\"time\",\"heartrate\")\n",
    "\n",
    "        write_stream = (df_final.writeStream\n",
    "                        .foreachBatch(upserter.upsertdata)\n",
    "                        .outputMode(\"append\")\n",
    "                        .option(\"queryName\",\"upsert_workout_bpm_stream\")\n",
    "                        .option(\"checkpointLocation\",f\"{self.checkpoint_base}/workout_bpm\")\n",
    "                        )\n",
    "        \n",
    "        if once:\n",
    "            write_stream.trigger(availableNow=True).start()\n",
    "        else:\n",
    "            write_stream.trigger(processingTime=process_time).start()\n",
    "    \n",
    "    def _await_queries(self, once):\n",
    "        if once:\n",
    "            for stream in spark.streams.active:\n",
    "                stream.awaitTermination()\n",
    "                \n",
    "    def upsert(self, once=True, processing_time=\"5 seconds\"):\n",
    "        import time\n",
    "        start = int(time.time())\n",
    "        print(f\"\\nExecuting silver layer upsert ...\")\n",
    "        self.upsert_user_details(once, processing_time)\n",
    "        self.upsert_gym_logs(once, processing_time)\n",
    "        self.upsert_user_profile(once, processing_time)\n",
    "        self.upsert_workouts(once, processing_time)\n",
    "        self.upsert_heart_rate(once, processing_time)        \n",
    "        self._await_queries(once)\n",
    "        print(f\"Completed silver layer 1 upsert {int(time.time()) - start} seconds\")\n",
    "        self.upsert_user_bins(once, processing_time)\n",
    "        self.upsert_completed_workouts(once, processing_time)        \n",
    "        self._await_queries(once)\n",
    "        print(f\"Completed silver layer 2 upsert {int(time.time()) - start} seconds\")\n",
    "        self.upsert_workout_bpm(once, processing_time)\n",
    "        self._await_queries(once)\n",
    "        print(f\"Completed silver layer 3 upsert {int(time.time()) - start} seconds\")\n",
    "        \n",
    "        \n",
    "    def assert_count(self, table_name, expected_count, filter=\"true\"):\n",
    "        print(f\"Validating record counts in {table_name}...\", end='')\n",
    "        actual_count = spark.read.table(f\"{self.catalog}.{self.silver_db}.{table_name}\").where(filter).count()\n",
    "        assert actual_count == expected_count, f\"Expected {expected_count:,} records, found {actual_count:,} in {table_name} where {filter}\" \n",
    "        print(f\"Found {actual_count:,} / Expected {expected_count:,} records where {filter}: Success\")        \n",
    "        \n",
    "    def validate(self, sets):\n",
    "        import time\n",
    "        start = int(time.time())\n",
    "        print(f\"\\nValidating silver layer records...\")\n",
    "        self.assert_count(\"users\", 5 if sets == 1 else 10)\n",
    "        self.assert_count(\"gym_logs\", 8 if sets == 1 else 16)\n",
    "        self.assert_count(\"user_profile\", 5 if sets == 1 else 10)\n",
    "        self.assert_count(\"workouts\", 16 if sets == 1 else 32)\n",
    "        self.assert_count(\"heart_rate\", sets * 253801)\n",
    "        self.assert_count(\"user_bins\", 5 if sets == 1 else 10)\n",
    "        self.assert_count(\"completed_workouts\", 8 if sets == 1 else 16)\n",
    "        self.assert_count(\"workout_bpm\", 3968 if sets == 1 else 8192)\n",
    "        print(f\"Silver layer validation completed in {int(time.time()) - start} seconds\")\n",
    "\n",
    "    \n",
    "\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "05-silver",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
